{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport spacy\nimport nltk\nimport re\nimport string\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install gdown\n!gdown 1VmpeZgh9reH3dUYRUlaqQsj2mh3hhdb-\n\n!wget https://nlp.stanford.edu/data/glove.6B.zip\n\n!unzip /content/glove.6B.zip\n\n!rm -rf /content/glove.6B.zip\n!rm /content/glove.6B.50d.txt\n!rm /content/glove.6B.100d.txt\n!rm /content/glove.6B.200d.txt\n\nclear_output()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nimport string\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words(\"english\")\n# stop_words_regex = re.compile(r'\\b(?:{})\\b'.format('|'.join(stop_words)))\npunctuations_regex = re.compile(r'[^\\w\\s]')\ndef tokenise(text):\n#     text = re.sub(stop_words_regex, \"\", text)\n    text = re.sub(punctuations_regex, \"\", text)\n    return text.split()\n\nreview_df[\"tokenised_review\"] = review_df[\"Verbatim Feedback \"].apply(tokenise)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, text, max_text_length=205):\n        self.text = text\n        self.max_text_length = max_text_length\n        self.vocab = self.create_vocab()\n        self.unk_index = 0\n        self.pad_index = 1\n    \n    def create_vocab(self):\n        vocab = []\n        for text in self.text:\n            for token in text:\n                if token not in vocab:\n                    vocab.append(token)\n        vocab.sort()\n        vocab_dict = {word:(index+2) for index, word in enumerate(vocab)}\n        vocab_dict[\"unk\"] = 0\n        vocab_dict[\"pad\"] = 1\n        return vocab_dict\n    \n    def vocab_size(self):\n        return len(self.vocab)\n        \n    def token2index(self, tokens):\n        embed = np.array([self.pad_index] * self.max_text_length)\n        for index, token in enumerate(tokens[:self.max_text_length]):\n            if token in self.vocab:\n                embed[index] = self.vocab[token]\n            else:\n                embed[index] = self.unk_index\n        return embed ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = review_df[\"tokenised_review\"].to_list()\ntrain_vocab = Vocab(text, max_text_length=10)\nvocab_size = train_vocab.vocab_size()\n\nreview_df[\"embedded_review\"] = review_df[\"tokenised_review\"].apply(lambda x: train_vocab.token2index(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = list(review_df[\"embedded_review\"])\ny = list(review_df[\"Sentiment (1=Positive & 0= Negative)\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class load_dataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.X[idx].astype(np.float32)), self.y[idx]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_dataset(X_train, y_train)\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, vocab_size, hidden_dim, output_dim, num_of_layers, embedding_dim=300, bidirectional=True):\n        super(LSTM, self).__init__()\n        self.bidirectional = bidirectional\n        \n        self.word2vec = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size=embedding_dim,\n                            hidden_size=hidden_dim, \n                            num_layers=num_of_layers,\n                            batch_first=True,\n                            dropout=0.2, \n                            bidirectional=self.bidirectional)\n        self.fc = nn.Sequential(nn.Linear(hidden_dim * 2, 256) if(bidirectional) else nn.Linear(hidden_dim, 256),\n                                nn.Dropout(),\n                                nn.Linear(256, 64),\n                                nn.Dropout(),\n                                nn.Linear(64, 16),\n                                nn.Dropout(),\n                                nn.Linear(16, 4),\n                                nn.Dropout(),\n                                nn.Linear(4, output_dim),\n                               )\n\n    def forward(self, text):\n        text = self.word2vec(text)\n        output, (hidden_state, cell_state) = self.lstm(text)\n\n        if(self.bidirectional):\n            hidden_state = torch.cat((hidden_state[-2, :, :], hidden_state[-1, : , :]), dim = 1)\n        else:\n            hidden_state = hidden_state[-1]\n        \n        outputs = self.fc(hidden_state)\n        \n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, device, epochs=20, lr=3e-4):\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_function = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loop = tqdm(enumerate(train_loader), total=len(train_loader), colour=\"green\")\n        for index, (X, y) in train_loop:\n            X = X.to(device).long()\n            y = y.to(device)\n            y_pred = model(X)\n            optimizer.zero_grad()\n            loss = loss_function(y_pred, torch.reshape(y, (-1, 1)).float())\n            loss.backward()\n            optimizer.step()\n            train_loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n            train_loop.set_postfix(loss=loss.item())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_dim = 512\noutput_dim = 1\nnum_of_layers = 2\nembedding_dim = 100\nlstm_model = LSTM(vocab_size, hidden_dim, output_dim, num_of_layers, embedding_dim)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nepochs = 20\nlr = 3e-4\ntrain_model(lstm_model, train_loader, device, epochs, lr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = load_dataset(X_test, y_test)\nbatch_size = 1\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntest_loop = tqdm(enumerate(test_loader), total=len(test_loader), colour=\"green\")\nfor index, (X, y) in test_loop:\n    X = X.to(device).long()\n    y = y.to(device)\n    y_pred = lstm_model(X)\n    if y_pred>=0.5:\n        predictions.append(1)\n    else:\n        predictions.append(0)\n    test_loop.set_description(\"Calculating accuracy..\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(predictions, y_test)","metadata":{},"execution_count":null,"outputs":[]}]}